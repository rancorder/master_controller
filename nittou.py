#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
nittou.py - æ—¥æ±å•†äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä¿®æ­£ç‰ˆï¼‰

ä¿®æ­£å†…å®¹:
- ğŸ”´ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼ˆtimeoutæœªæŒ‡å®šï¼‰
- ğŸ”´ ç„¡é™ãƒ«ãƒ¼ãƒ—å¯¾ç­–ï¼ˆMAX_PAGESä¸Šé™ï¼‰
- ğŸ”´ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ˜ç¤ºåŒ–
- ğŸŸ¡ ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹è¿½åŠ 
- ğŸŸ¡ é‡è¤‡æ’é™¤æ©Ÿæ§‹
- ğŸŸ¢ url_indexå¯¾å¿œ
- ğŸŸ¢ ã‚»ãƒƒã‚·ãƒ§ãƒ³å†åˆ©ç”¨
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
from typing import List, Dict

# å®šæ•°
MAX_PAGES = 50
TIMEOUT_SECONDS = 10
RETRY_COUNT = 3
RETRY_DELAY = 2

def scrape_nittou() -> List[Dict[str, str]]:
    """æ—¥æ±å•†äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    
    Returns:
        List[Dict[str, str]]: å•†å“ãƒªã‚¹ãƒˆ [{"name": ..., "price": ...}, ...]
    """
    
    print(f"nittou.py å®Ÿè¡Œé–‹å§‹: {datetime.now()}")
    
    results = []
    seen = set()  # é‡è¤‡æ’é™¤ç”¨
    page = 0
    
    with requests.Session() as session:
        # User-Agentè¨­å®šï¼ˆãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ï¼‰
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        while page < MAX_PAGES:
            url = (
                f"https://camerafan.jp/nittou/itemlist.php?"
                f"m=&c=0&s=nittou&l=&h=&w=&nw=&sr=-16&re=0&p={page}&sp=&max=50"
            )
            
            # ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
            for retry in range(RETRY_COUNT):
                try:
                    response = session.get(url, timeout=TIMEOUT_SECONDS)
                    response.raise_for_status()  # HTTP 4xx/5xxã‚’ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†
                    
                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ˜ç¤º
                    response.encoding = 'utf-8'
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    break  # æˆåŠŸã—ãŸã‚‰ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    
                except requests.Timeout:
                    print(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ãƒšãƒ¼ã‚¸={page}, ãƒªãƒˆãƒ©ã‚¤={retry+1}/{RETRY_COUNT})")
                    if retry < RETRY_COUNT - 1:
                        time.sleep(RETRY_DELAY)
                    else:
                        print(f"âŒ {RETRY_COUNT}å›ãƒªãƒˆãƒ©ã‚¤å¤±æ•—, ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        page += 1
                        break
                        
                except requests.RequestException as e:
                    print(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ (ãƒšãƒ¼ã‚¸={page}): {e}")
                    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å…¨ä½“çµ‚äº†
                    return results
            else:
                # ãƒªãƒˆãƒ©ã‚¤å¤±æ•—ã§æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
                continue
            
            # å•†å“æŠ½å‡º
            items = soup.select("div#items div.item")
            
            if not items:
                print(f"  ãƒšãƒ¼ã‚¸{page}: å•†å“ãªã—ï¼ˆæœ€çµ‚ãƒšãƒ¼ã‚¸ï¼‰")
                break
            
            page_count = 0
            for item in items:
                try:
                    name_tag = item.select_one("div.itemn a")
                    price_tag = item.select_one("div.itemp")
                    
                    if not name_tag or not price_tag:
                        continue
                    
                    name = name_tag.get_text(strip=True)
                    price = price_tag.get_text(strip=True).replace('Â¥', '').replace(',', '').strip()
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    key = f"{name.lower()}_{price}"
                    if key in seen:
                        continue
                    
                    seen.add(key)
                    results.append({"name": name, "price": price})
                    page_count += 1
                    
                except Exception as e:
                    print(f"âš ï¸ å•†å“ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                    continue
            
            print(f"  ãƒšãƒ¼ã‚¸{page}: {page_count}ä»¶å–å¾—")
            
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ç¢ºèª
            next_link = soup.find("a", string=lambda s: s and "æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸" in s)
            if not next_link:
                print(f"  æœ€çµ‚ãƒšãƒ¼ã‚¸åˆ°é”ï¼ˆ{page}ãƒšãƒ¼ã‚¸ç›®ï¼‰")
                break
            
            page += 1
    
    print(f"âœ… å–å¾—å®Œäº†: {len(results)}ä»¶ï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰")
    return results


if __name__ == "__main__":
    items = scrape_nittou()
    
    # master_controllerç”¨ã®æ¨™æº–å‡ºåŠ›
    for item in items:
        print(f"{item['name']} {item['price']}å††")