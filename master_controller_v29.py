#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Master Controller v27 - P1å€‹åˆ¥ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¯¾å¿œç‰ˆï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ¸ˆï¼‰

ã€v27æ”¹å–„ç‚¹ã€‘
ğŸ”§ P1ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ã®å€‹åˆ¥JSONãƒ•ã‚¡ã‚¤ãƒ«åŒ–ï¼ˆç«¶åˆè§£æ¶ˆï¼‰
ğŸ”§ snapshotsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®çµ±åˆé…ç½®
ğŸ”§ P2ã¯å…±æœ‰ãƒ•ã‚¡ã‚¤ãƒ«ç¶­æŒï¼ˆp2_shared.jsonï¼‰
ğŸ”§ å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç›£è¦–ï¼ˆ30åˆ†ã”ã¨ï¼‰
ğŸ”§ ãƒãƒ¼ãƒ‰ã‚ªãƒ•å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ

ã€ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã€‘
snapshots/
  p1_{script}_{url_index}.json  â† P1å€‹åˆ¥
  p2_shared.json                â† P2å…±æœ‰
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
import random
import re
import shutil
import signal
import sqlite3
import subprocess
import sys
import threading
import time
import unicodedata
from contextlib import contextmanager, suppress
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum, auto
from functools import lru_cache
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Final

import pandas as pd
import requests

# ==================== ç’°å¢ƒå¤‰æ•°è¨­å®š ====================
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTHONDONTWRITEBYTECODE'] = '1'

# ==================== è¨­å®šå®šæ•° ====================
SUBPROCESS_TIMEOUT: Final[int] = int(os.getenv('SCRAPER_TIMEOUT', '120'))
HTTP_TIMEOUT: Final[int] = int(os.getenv('HTTP_TIMEOUT', '10'))
DB_TIMEOUT: Final[float] = 30.0

USE_SQLITE_HISTORY: Final[bool] = True

# ==================== ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ====================
SNAPSHOTS_DIR: Final[Path] = Path("snapshots")
P2_SHARED_SNAPSHOT: Final[str] = "p2_shared.json"

# ==================== ãƒ­ã‚°è¨­å®š ====================
log_handler_file = RotatingFileHandler(
    'master_controller.log',
    maxBytes=10*1024*1024,
    backupCount=2,
    encoding='utf-8'
)
log_handler_file.setFormatter(
    logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
)

log_handler_stdout = logging.StreamHandler(sys.stdout)
log_handler_stdout.setFormatter(
    logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
)

logging.basicConfig(
    level=logging.INFO,
    handlers=[log_handler_file, log_handler_stdout]
)

LOGGER = logging.getLogger('MasterController')


# ==================== ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµ±ä¸€ ====================

class ErrorSeverity(Enum):
    """ã‚¨ãƒ©ãƒ¼é‡è¦åº¦ã®åˆ†é¡"""
    RECOVERABLE = auto()
    EXPECTED = auto()
    FATAL = auto()


class ErrorHandler:
    """ã‚¨ãƒ©ãƒ¼å‡¦ç†æˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    
    @staticmethod
    def handle(error: Exception, context: str, severity: ErrorSeverity) -> None:
        if severity == ErrorSeverity.RECOVERABLE:
            LOGGER.warning(f"[{context}] ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã‚¨ãƒ©ãƒ¼: {error}")
        elif severity == ErrorSeverity.EXPECTED:
            LOGGER.info(f"[{context}] æƒ³å®šå†…ã‚¨ãƒ©ãƒ¼: {error}")
        elif severity == ErrorSeverity.FATAL:
            LOGGER.critical(f"[{context}] è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {error}", exc_info=True)
            sys.exit(1)


# ==================== ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ ====================

class TimeSlot(Enum):
    """æ™‚é–“å¸¯ã®åˆ†é¡"""
    DAYTIME = "daytime"
    NIGHTTIME = "nighttime"


class Priority(Enum):
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆå„ªå…ˆåº¦"""
    HIGH = 1
    LOW = 2


@dataclass(frozen=True)
class ScraperConfig:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    py_file: str
    display_name: str
    category: str
    scraping_url: str
    url_index: int
    priority: Priority
    is_active: bool
    notification_room_ids: Optional[str] = None


@dataclass(frozen=True)
class ProductData:
    """å•†å“ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    price: str
    site_name: str
    url: str
    url_index: int
    img_url: str = ""


@dataclass
class ExecutionResult:
    """å®Ÿè¡Œçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    duration: float = 0.0
    error: Optional[str] = None
    data: Dict[str, Any] = field(default_factory=dict)


@dataclass(frozen=True)
class SnapshotStatus:
    """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆçŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    site: str
    priority: str
    elapsed_minutes: float
    is_fresh: bool
    file: str


# ==================== æ™‚é–“å¸¯ç®¡ç† ====================

class TimeManager:
    """æ™‚é–“å¸¯ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    NIGHT_START_HOUR: Final[int] = 1
    NIGHT_END_HOUR: Final[int] = 8
    NIGHT_INTERVAL_SECONDS: Final[int] = 1800
    
    @classmethod
    def get_current_timeslot(cls) -> TimeSlot:
        current_hour = datetime.now().hour
        if cls.NIGHT_START_HOUR <= current_hour < cls.NIGHT_END_HOUR:
            return TimeSlot.NIGHTTIME
        return TimeSlot.DAYTIME
    
    @classmethod
    def is_nighttime(cls) -> bool:
        return cls.get_current_timeslot() == TimeSlot.NIGHTTIME
    
    @classmethod
    def get_interval_for_priority1(cls, idle_seconds: float, is_night: bool) -> int:
        if is_night:
            return cls.NIGHT_INTERVAL_SECONDS
        if idle_seconds < 1800:
            return 60
        elif idle_seconds < 3600:
            return 300
        else:
            return 3600
    
    @classmethod
    def get_interval_for_priority2(cls, is_night: bool) -> int:
        if is_night:
            return cls.NIGHT_INTERVAL_SECONDS
        return 300


# ==================== ãƒ‡ãƒ¼ã‚¿æŠ½å‡º ====================

class StableDataExtractor:
    """æ¨™æº–å‡ºåŠ›ã‹ã‚‰ã®å•†å“ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå™¨"""
    
    SKIP_KEYWORDS: Final[List[str]] = [
        'info', 'error', 'debug', 'warning', 'log', 'traceback',
        'selenium', 'driver', 'browser', 'playwright'
    ]
    
    PRICE_PATTERNS: Final[List[re.Pattern]] = [
        re.compile(r'([0-9,]+)\s*å††'),
        re.compile(r'Â¥\s*([0-9,]+)'),
        re.compile(r'(\d{4,})\s*å††')
    ]
    
    MAX_OUTPUT_SIZE: Final[int] = 1_000_000
    
    def __init__(self, logger: logging.Logger = LOGGER) -> None:
        self.logger = logger
    
    def extract_stable(self, output: str, script_name: str) -> Dict[str, Any]:
        if len(output) > self.MAX_OUTPUT_SIZE:
            raise ValueError(f"å‡ºåŠ›ã‚µã‚¤ã‚ºè¶…é: {len(output)} > {self.MAX_OUTPUT_SIZE}")
        
        if not output or len(output.strip()) < 10:
            return {'count': 0, 'success': False, 'products': []}
        
        script_display = script_name.replace('.py', '')
        products: List[Dict[str, Any]] = []
        lines = output.split('\n')
        current_url_index = 0
        
        for line in lines:
            line = line.strip()
            if len(line) < 10:
                continue
            
            if line.startswith("---URL_INDEX:"):
                match = re.search(r'---URL_INDEX:(\d+)---', line)
                if match:
                    current_url_index = int(match.group(1))
                continue
            
            if self._should_skip_line(line):
                continue
            
            product = self._extract_product_from_line(
                line, script_display, current_url_index
            )
            if product:
                products.append(product)
        
        product_count = len(products)
        success = product_count > 0
        
        if success:
            self.logger.info(f"[{script_display}] ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {product_count}ä»¶")
        else:
            self.logger.warning(f"[{script_display}] ãƒ‡ãƒ¼ã‚¿å–å¾—0ä»¶")
        
        return {
            'count': product_count,
            'success': success,
            'products': products
        }
    
    def _should_skip_line(self, line: str) -> bool:
        line_lower = line.lower()
        return any(word in line_lower for word in self.SKIP_KEYWORDS)
    
    def _extract_product_from_line(
        self, line: str, site_name: str, url_index: int
    ) -> Optional[Dict[str, Any]]:
        img_url = ""
        name_price_part = line
        
        if '||' in line:
            parts = line.split('||')
            name_price_part = parts[0]
            img_url = parts[1] if len(parts) > 1 else ""
        
        price = self._extract_price(name_price_part)
        if not price:
            return None
        
        product_name = self._extract_product_name(name_price_part)
        if len(product_name) <= 3:
            return None
        
        return {
            'name': product_name[:200],
            'price': str(price),
            'site_name': site_name,
            'url': 'N/A',
            'url_index': url_index,
            'img_url': img_url
        }
    
    def _extract_price(self, text: str) -> Optional[int]:
        for pattern in self.PRICE_PATTERNS:
            match = pattern.search(text)
            if match:
                price_text = match.group(1).replace(',', '')
                try:
                    price = int(price_text)
                    if 100 <= price <= 10_000_000:
                        return price
                except ValueError:
                    continue
        return None
    
    def _extract_product_name(self, text: str) -> str:
        product_name = text
        for pattern in self.PRICE_PATTERNS:
            product_name = pattern.sub('', product_name)
        product_name = re.sub(r'\s+', ' ', product_name).strip()
        product_name = re.sub(r'[|â”‚]+', ' ', product_name).strip()
        return product_name


# ==================== é€šçŸ¥å±¥æ­´ç®¡ç† ====================

class NotificationHistoryManager:
    """é€šçŸ¥å±¥æ­´ç®¡ç†(æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹)"""
    
    def should_notify(self, product_key: str, cooldown_hours: int = 6) -> bool:
        raise NotImplementedError
    
    def add_notification(self, product_key: str, site_name: str) -> None:
        raise NotImplementedError
    
    def cleanup(self, retention_hours: int = 24) -> int:
        raise NotImplementedError


class SQLiteNotificationHistory(NotificationHistoryManager):
    """SQLiteå½¢å¼ã®é€šçŸ¥å±¥æ­´"""
    
    def __init__(
        self,
        db_path: str = "notification_history.db",
        logger: logging.Logger = LOGGER
    ) -> None:
        self.db_path = db_path
        self.logger = logger
        self._init_db()
    
    def _init_db(self) -> None:
        conn = None
        try:
            conn = sqlite3.connect(
                self.db_path,
                timeout=DB_TIMEOUT,
                isolation_level=None,
                check_same_thread=False
            )
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=-64000")
            conn.execute("BEGIN")
            conn.execute("""
                CREATE TABLE IF NOT EXISTS notifications (
                    product_key TEXT PRIMARY KEY,
                    site_name TEXT NOT NULL,
                    notified_at TIMESTAMP NOT NULL
                ) WITHOUT ROWID
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_notified_at
                ON notifications(notified_at)
            """)
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                conn.close()
        
        self.logger.info("SQLiteé€šçŸ¥å±¥æ­´åˆæœŸåŒ–å®Œäº†(WALãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹)")
    
    @contextmanager
    def _get_connection(self):
        conn = None
        max_retries = 10
        base_delay = 0.05
        
        for attempt in range(max_retries):
            try:
                conn = sqlite3.connect(
                    self.db_path,
                    timeout=DB_TIMEOUT,
                    isolation_level='DEFERRED',
                    detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
                    check_same_thread=False
                )
                sqlite3.register_adapter(datetime, lambda val: val.isoformat())
                sqlite3.register_converter(
                    "TIMESTAMP",
                    lambda val: datetime.fromisoformat(val.decode())
                )
                conn.execute("PRAGMA busy_timeout=30000")
                conn.execute("BEGIN")
                yield conn
                conn.commit()
                return
            except sqlite3.OperationalError as e:
                if conn:
                    try:
                        conn.rollback()
                    except:
                        pass
                if "locked" in str(e).lower() and attempt < max_retries - 1:
                    jitter = random.uniform(0, 0.1)
                    wait_time = min(base_delay * (2 ** attempt) + jitter, 5.0)
                    self.logger.warning(
                        f"SQLite locked (è©¦è¡Œ{attempt+1}/{max_retries}): "
                        f"{wait_time:.2f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤"
                    )
                    time.sleep(wait_time)
                    continue
                raise
            except Exception:
                if conn:
                    try:
                        conn.rollback()
                    except:
                        pass
                raise
            finally:
                if conn:
                    try:
                        conn.close()
                    except:
                        pass
    
    def should_notify(self, product_key: str, cooldown_hours: int = 6) -> bool:
        cutoff = datetime.now() - timedelta(hours=cooldown_hours)
        try:
            with self._get_connection() as conn:
                result = conn.execute(
                    "SELECT notified_at FROM notifications "
                    "WHERE product_key = ? AND notified_at > ?",
                    (product_key, cutoff)
                ).fetchone()
                return result is None
        except Exception as e:
            ErrorHandler.handle(e, "é€šçŸ¥åˆ¤å®š", ErrorSeverity.EXPECTED)
            return False
    
    def add_notification(self, product_key: str, site_name: str) -> None:
        try:
            with self._get_connection() as conn:
                conn.execute(
                    "INSERT OR REPLACE INTO notifications "
                    "(product_key, site_name, notified_at) VALUES (?, ?, ?)",
                    (product_key, site_name, datetime.now())
                )
        except Exception as e:
            ErrorHandler.handle(e, "é€šçŸ¥å±¥æ­´è¿½åŠ ", ErrorSeverity.RECOVERABLE)
    
    def cleanup(self, retention_hours: int = 24) -> int:
        cutoff = datetime.now() - timedelta(hours=retention_hours)
        try:
            with self._get_connection() as conn:
                cursor = conn.execute(
                    "DELETE FROM notifications WHERE notified_at < ?",
                    (cutoff,)
                )
                deleted_count = cursor.rowcount
                if deleted_count > 0:
                    self.logger.debug(f"ğŸ—‘ï¸ å¤ã„é€šçŸ¥å±¥æ­´å‰Šé™¤: {deleted_count}ä»¶")
                if deleted_count > 100:
                    conn.execute("VACUUM")
                return deleted_count
        except Exception as e:
            ErrorHandler.handle(e, "é€šçŸ¥å±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—", ErrorSeverity.EXPECTED)
            return 0


# ==================== å·®åˆ†æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ (v26æ”¹ä¿®ç‰ˆ) ====================

class SimpleMemoryDiffSystem:
    """ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹å·®åˆ†æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ (v26: P1å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ)"""
    
    NOTIFICATION_COOLDOWN_HOURS: Final[int] = 6
    CLEANUP_THRESHOLD_HOURS: Final[int] = 24
    SNAPSHOT_THRESHOLD_MINUTES: Final[int] = 30
    
    NOISE_WORDS: Final[List[str]] = ['æ–°ç€!!', 'æ–°ç€', 'å€¤ä¸‹', 'ç¾å“', 'æ¥µä¸Šå“', 'è‰¯å“', 'ä¸¦å“']
    
    def __init__(
        self,
        snapshot_dir: Path = SNAPSHOTS_DIR,
        logger: logging.Logger = LOGGER
    ) -> None:
        self.snapshot_dir = snapshot_dir
        self.logger = logger
        self.last_snapshots: Dict[str, Dict[str, Any]] = {}
        self.file_locks: Dict[str, threading.RLock] = {}
        self.global_lock = threading.RLock()
        
        self.notification_manager = SQLiteNotificationHistory(logger=logger)
        
        # æ­£è¦è¡¨ç¾ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        self.BRACKET_PATTERN = re.compile(r'[\[ã€Œ\(].*?[\]ã€\)]')
        self.NONWORD_PATTERN = re.compile(r'[^\w]')
        noise_escaped = '|'.join(map(re.escape, self.NOISE_WORDS))
        self.NOISE_PATTERN = re.compile(noise_escaped, re.IGNORECASE)
        
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self._ensure_snapshot_dir()
        
        # P1/P2ã®ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°(å¾Œã§è¨­å®šã•ã‚Œã‚‹)
        self.priority_mapping: Dict[str, int] = {}
        
        self.logger.info(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.snapshot_dir}")
        self.logger.info("é€šçŸ¥å±¥æ­´: SQLiteå½¢å¼(æ¨å¥¨ç‰ˆãƒ»ACIDä¿è¨¼)")
    
    def _ensure_snapshot_dir(self) -> None:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        if not self.snapshot_dir.exists():
            self.snapshot_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"ğŸ“ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {self.snapshot_dir}")
    
    def set_priority_mapping(self, mapping: Dict[str, int]) -> None:
        """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã”ã¨ã®ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¨­å®š"""
        self.priority_mapping = mapping
        self.logger.info(f"ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š: P1={sum(1 for v in mapping.values() if v == 1)}ä»¶, P2={sum(1 for v in mapping.values() if v == 2)}ä»¶")
    
    def _get_file_lock(self, filepath: str) -> threading.RLock:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒ­ãƒƒã‚¯ã‚’å–å¾—"""
        with self.global_lock:
            if filepath not in self.file_locks:
                self.file_locks[filepath] = threading.RLock()
            return self.file_locks[filepath]
    
    def _get_snapshot_path(self, site_name: str, script_name: str, url_index: int) -> Path:
        """
        ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        
        P1: snapshots/p1_{script}_{url_index}.json(å€‹åˆ¥)
        P2: snapshots/p2_shared.json(å…±æœ‰)
        """
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆåã‹ã‚‰ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ã‚’åˆ¤å®š
        priority = self.priority_mapping.get(script_name, 2)
        
        if priority == 1:
            # P1ã¯å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
            safe_script = script_name.replace('.py', '').replace('/', '_').replace(' ', '_')
            filename = f"p1_{safe_script}_{url_index}.json"
            self.logger.debug(f"[{site_name}] P1å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        else:
            # P2ã¯å…±æœ‰ãƒ•ã‚¡ã‚¤ãƒ«
            filename = P2_SHARED_SNAPSHOT
            self.logger.debug(f"[{site_name}] P2å…±æœ‰ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        
        return self.snapshot_dir / filename
    
    def _load_snapshot_file(self, filepath: Path) -> Dict[str, Dict[str, Any]]:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not filepath.exists():
            return {}
        
        file_lock = self._get_file_lock(str(filepath))
        try:
            with file_lock:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except json.JSONDecodeError as e:
            ErrorHandler.handle(e, f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿({filepath.name})", ErrorSeverity.EXPECTED)
            return {}
        except Exception as e:
            ErrorHandler.handle(e, f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿({filepath.name})", ErrorSeverity.EXPECTED)
            return {}
    
    def _save_snapshot_file(self, filepath: Path, data: Dict[str, Dict[str, Any]]) -> None:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜(ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿)"""
        file_lock = self._get_file_lock(str(filepath))
        try:
            with file_lock:
                temp_path = filepath.with_suffix('.tmp')
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    f.flush()
                    os.fsync(f.fileno())
                shutil.move(str(temp_path), str(filepath))
        except Exception as e:
            ErrorHandler.handle(e, f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜({filepath.name})", ErrorSeverity.RECOVERABLE)
            with suppress(OSError):
                temp_path = filepath.with_suffix('.tmp')
                if temp_path.exists():
                    temp_path.unlink()
    
    def detect_new_products(
        self,
        site_name: str,
        products: List[Dict[str, Any]],
        scraping_url: str = '',
        script_name: str = '',
        url_index: int = 0
    ) -> List[Dict[str, Any]]:
        """æ–°å•†å“ã‚’æ¤œå‡º(v26: å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ)"""
        if not products:
            self.logger.warning(f"[{site_name}] å•†å“ãƒ‡ãƒ¼ã‚¿ãªã—")
            return []
        
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        snapshot_path = self._get_snapshot_path(site_name, script_name, url_index)
        
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿
        snapshot_data = self._load_snapshot_file(snapshot_path)
        
        is_first_run = site_name not in snapshot_data
        
        if is_first_run:
            return self._handle_first_run(site_name, products, scraping_url, snapshot_path, snapshot_data)
        
        snapshot = snapshot_data.get(site_name, {})
        remembered_first_key = snapshot.get('first_product_key')
        remembered_name = snapshot.get('first_product_name', 'ä¸æ˜')
        
        current_first_key = self._normalize_product_key(products[0])
        if current_first_key == remembered_first_key:
            self.logger.info(f"[{site_name}] âœ… å¤‰æ›´ãªã—(1ä½ã¯åŒã˜)")
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ã¿æ›´æ–°
            if site_name in snapshot_data:
                snapshot_data[site_name]['timestamp'] = datetime.now().isoformat()
                self._save_snapshot_file(snapshot_path, snapshot_data)
                self.logger.debug(f"[{site_name}] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ã¿æ›´æ–°")
            
            return []
        
        previous_first_position = None
        for idx, product in enumerate(products):
            if self._normalize_product_key(product) == remembered_first_key:
                previous_first_position = idx
                break
        
        self.logger.info(f"[{site_name}] å‰å›1ä½: {remembered_name[:50]}")
        self.logger.info(f"   å‰å›ãƒãƒƒã‚·ãƒ¥: {remembered_first_key}")
        
        if previous_first_position is None:
            new_products = products[:20]
            self.logger.info(
                f"[{site_name}] ğŸ‰ å‰å›1ä½æ¶ˆå¤±: ä¸Šä½{len(new_products)}ä»¶ã‚’æ–°å•†å“ã¨ã—ã¦æ¤œçŸ¥"
            )
            for i, p in enumerate(new_products, 1):
                self.logger.info(f"   æ–°{i}ä½: {p['name'][:50]} / {p.get('price', '0')}å††")
        elif previous_first_position == 0:
            new_products = []
        else:
            new_products = products[:previous_first_position]
            self.logger.info(
                f"[{site_name}] ğŸ‰ æ–°å•†å“æ¤œçŸ¥: {len(new_products)}ä»¶ãŒä¸Šä½ã«æŒ¿å…¥"
            )
            for i, p in enumerate(new_products, 1):
                self.logger.info(f"   æ–°{i}ä½: {p['name'][:50]} / {p.get('price', '0')}å††")
            self.logger.info(
                f"   å‰å›1ä½ã¯ç¾åœ¨{previous_first_position + 1}ä½ã«å¾Œé€€"
            )
        
        self._update_snapshot(site_name, products[0], scraping_url, snapshot_path, snapshot_data)
        
        return self._apply_notification_cooldown(site_name, new_products)
    
    def _handle_first_run(
        self,
        site_name: str,
        products: List[Dict[str, Any]],
        scraping_url: str,
        snapshot_path: Path,
        snapshot_data: Dict[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """åˆå›å®Ÿè¡Œæ™‚ã®å‡¦ç†"""
        first_product = products[0]
        first_key = self._normalize_product_key(first_product)
        first_name = first_product['name']
        first_price = first_product.get('price', '0')
        
        snapshot_data[site_name] = {
            'first_product_key': first_key,
            'first_product_name': first_name,
            'first_product_price': first_price,
            'first_product_url': scraping_url,
            'timestamp': datetime.now().isoformat()
        }
        self._save_snapshot_file(snapshot_path, snapshot_data)
        
        price_text = f"{first_price}å††" if first_price != '0' else "ãŠå•ã„åˆã‚ã›"
        
        self.logger.info(f"[{site_name}] åˆå›å®Ÿè¡Œ: 1ä½ã‚’è¨˜æ†¶(é€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—)")
        self.logger.info(f"   å•†å“å: {first_name[:50]}")
        self.logger.info(f"   ä¾¡æ ¼: {price_text}")
        self.logger.info(f"   ãƒãƒƒã‚·ãƒ¥: {first_key}")
        self.logger.info(f"   ä¿å­˜å…ˆ: {snapshot_path.name}")
        
        return []
    
    def _update_snapshot(
        self,
        site_name: str,
        current_first: Dict[str, Any],
        scraping_url: str,
        snapshot_path: Path,
        snapshot_data: Dict[str, Dict[str, Any]]
    ) -> None:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ›´æ–°"""
        first_key = self._normalize_product_key(current_first)
        first_name = current_first['name']
        first_price = current_first.get('price', '0')
        
        snapshot_data[site_name] = {
            'first_product_key': first_key,
            'first_product_name': first_name,
            'first_product_price': first_price,
            'first_product_url': scraping_url,
            'timestamp': datetime.now().isoformat()
        }
        self._save_snapshot_file(snapshot_path, snapshot_data)
    
    def _apply_notification_cooldown(
        self,
        site_name: str,
        new_products: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """é€šçŸ¥ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’é©ç”¨"""
        if not new_products:
            return []
        
        notifiable_products = []
        
        for product in new_products:
            product_key = self._normalize_product_key(product)
            
            should_notify = self.notification_manager.should_notify(
                product_key,
                self.NOTIFICATION_COOLDOWN_HOURS
            )
            
            if should_notify:
                self.notification_manager.add_notification(product_key, site_name)
                notifiable_products.append(product)
            else:
                self.logger.info(
                    f"â¸ï¸ [{site_name}] é‡è¤‡é€šçŸ¥é˜²æ­¢: {product['name'][:30]}... ã‚’ã‚¹ã‚­ãƒƒãƒ—"
                )
        
        if len(notifiable_products) > 0:
            self.notification_manager.cleanup(self.CLEANUP_THRESHOLD_HOURS)
        
        return notifiable_products
    
    @lru_cache(maxsize=10000)
    def _normalize_product_name_cached(self, product_name: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãæ­£è¦åŒ–"""
        name = unicodedata.normalize('NFKC', product_name)
        name = self.BRACKET_PATTERN.sub('', name)
        name = self.NOISE_PATTERN.sub('', name)
        name = self.NONWORD_PATTERN.sub('', name.lower())
        return name
    
    def _normalize_product_key(self, product: Dict[str, Any]) -> str:
        """å•†å“ã‚­ãƒ¼ã‚’æ­£è¦åŒ–"""
        name = product.get('name', '')
        
        code_match = re.search(r'[A-Z0-9]{8,}', name.upper())
        if code_match:
            product_key = code_match.group()
            return hashlib.md5(product_key.encode('utf-8')).hexdigest()[:8]
        
        if product.get('img_url'):
            img_url = product['img_url'].split('?')[0]
            return hashlib.md5(img_url.encode('utf-8')).hexdigest()[:8]
        
        normalized = self._normalize_product_name_cached(name)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()[:8]
    
    def get_snapshot_stats(self) -> Dict[str, Any]:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆçµ±è¨ˆã‚’å–å¾—"""
        total_sites = 0
        p1_files = list(self.snapshot_dir.glob("p1_*.json"))
        p2_file = self.snapshot_dir / P2_SHARED_SNAPSHOT
        
        for p1_file in p1_files:
            data = self._load_snapshot_file(p1_file)
            total_sites += len(data)
        
        if p2_file.exists():
            data = self._load_snapshot_file(p2_file)
            total_sites += len(data)
        
        return {
            'total_sites': total_sites,
            'total_products': total_sites,
            'p1_files': len(p1_files),
            'p2_shared': p2_file.exists()
        }


# ==================== ãƒãƒ¼ãƒ‰ã‚ªãƒ•å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ (æ–°è¦) ====================

class HardOffFormatter:
    """ãƒãƒ¼ãƒ‰ã‚ªãƒ•å°‚ç”¨å•†å“ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    
    å¤‰æ›ä¾‹ï¼ˆã‚«ãƒ¡ãƒ©ç³»ã®ã¿ï¼‰:
    CANON ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒ‡ã‚¸ã‚«ãƒ¡ [IXY 650]ãƒ»34800å††
    â†“
    â– ã€IXY 650ãƒ»34800å††ã€‘CANON ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãƒ‡ã‚¸ã‚«ãƒ¡
    
    æ™‚è¨ˆç³»ã¯å¾“æ¥å½¢å¼ã®ã¾ã¾:
    OMEGA DEVILLE ã‚¯ã‚©ãƒ¼ãƒ„ [195.0075.2]ãƒ»57200å††
    â†“
    â–ª OMEGA DEVILLE ã‚¯ã‚©ãƒ¼ãƒ„ [195.0075.2]ãƒ»57200å††
    """
    
    CODE_PATTERN: Final[re.Pattern] = re.compile(r'\[([^\]]+)\]')
    
    # ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã®æ™‚è¨ˆURL index (å¾“æ¥å½¢å¼ã‚’ç¶­æŒ)
    HARDOFF_WATCH_URL_INDEX: Final[int] = 5
    
    @classmethod
    def is_hardoff(cls, display_name: str, scraping_url: str) -> bool:
        """ãƒãƒ¼ãƒ‰ã‚ªãƒ•åˆ¤å®š"""
        return "ãƒãƒ¼ãƒ‰ã‚ªãƒ•" in display_name or "hardoff" in scraping_url.lower()
    
    @classmethod
    def should_use_new_format(
        cls,
        display_name: str,
        scraping_url: str,
        url_index: int
    ) -> bool:
        """æ–°å½¢å¼ï¼ˆå•†å“ã‚³ãƒ¼ãƒ‰å…ˆé ­ï¼‰ã‚’ä½¿ç”¨ã™ã¹ãã‹åˆ¤å®š
        
        Args:
            display_name: ã‚µã‚¤ãƒˆè¡¨ç¤ºå
            scraping_url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
            url_index: URL index (0-5)
        
        Returns:
            True: æ–°å½¢å¼ï¼ˆã‚«ãƒ¡ãƒ©ç³»ï¼‰
            False: å¾“æ¥å½¢å¼ï¼ˆæ™‚è¨ˆç³» or ãƒãƒ¼ãƒ‰ã‚ªãƒ•ä»¥å¤–ï¼‰
        """
        # ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã§ãªã‘ã‚Œã°å¾“æ¥å½¢å¼
        if not cls.is_hardoff(display_name, scraping_url):
            return False
        
        # ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã®æ™‚è¨ˆï¼ˆurl_index=5ï¼‰ã¯å¾“æ¥å½¢å¼
        if url_index == cls.HARDOFF_WATCH_URL_INDEX:
            return False
        
        # ãã‚Œä»¥å¤–ã®ãƒãƒ¼ãƒ‰ã‚ªãƒ•ï¼ˆã‚«ãƒ¡ãƒ©ç³»: url_index=0,1,2,3,4ï¼‰ã¯æ–°å½¢å¼
        return True
    
    @classmethod
    def format_product_line(cls, product: Dict[str, Any]) -> str:
        """å•†å“è¡Œã‚’ãƒãƒ¼ãƒ‰ã‚ªãƒ•å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        name = product['name']
        price = product.get('price', '0')
        
        # å•†å“åã‹ã‚‰ [å•†å“ã‚³ãƒ¼ãƒ‰] ã‚’æŠ½å‡º
        code_match = cls.CODE_PATTERN.search(name)
        if code_match:
            code = code_match.group(1)
            # å•†å“ã‚³ãƒ¼ãƒ‰ã‚’é™¤ã„ãŸæ®‹ã‚Šã®åå‰
            name_without_code = cls.CODE_PATTERN.sub('', name).strip()
            
            price_text = f"{price}å††" if price != '0' else "ãŠå•ã„åˆã‚ã›"
            return f"â– ã€{code}ãƒ»{price_text}ã€‘{name_without_code}\n\n"
        else:
            # ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯å¾“æ¥å½¢å¼
            price_text = f"{price}å††" if price != '0' else "ãŠå•ã„åˆã‚ã›"
            return f"â–ª {name}ãƒ»{price_text}\n\n"


# ==================== ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨ (æ–°è¦) ====================

class SnapshotReportGenerator:
    """P1/P2å…¨URLã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå°‚é–€ã‚¯ãƒ©ã‚¹"""
    
    SNAPSHOT_THRESHOLD_SECONDS: Final[int] = 1800  # 30åˆ†
    
    def __init__(
        self,
        memory_system: SimpleMemoryDiffSystem,
        logger: logging.Logger = LOGGER
    ) -> None:
        self.memory_system = memory_system
        self.logger = logger
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """P1/P2å…¨URLã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        current_time = datetime.now()
        all_status: List[SnapshotStatus] = []
        
        # P1å€‹åˆ¥JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        p1_files = list(self.memory_system.snapshot_dir.glob("p1_*.json"))
        for p1_file in p1_files:
            data = self.memory_system._load_snapshot_file(p1_file)
            for site_name, snapshot in data.items():
                status = self._create_snapshot_status(
                    site_name, snapshot, current_time, 'P1', p1_file.name
                )
                if status:
                    all_status.append(status)
        
        # P2å…±æœ‰JSONèª­ã¿è¾¼ã¿
        p2_file = self.memory_system.snapshot_dir / P2_SHARED_SNAPSHOT
        if p2_file.exists():
            data = self.memory_system._load_snapshot_file(p2_file)
            for site_name, snapshot in data.items():
                status = self._create_snapshot_status(
                    site_name, snapshot, current_time, 'P2', P2_SHARED_SNAPSHOT
                )
                if status:
                    all_status.append(status)
        
        # æ–°é®®åº¦ã§ã‚½ãƒ¼ãƒˆ
        fresh_sites = [s for s in all_status if s.is_fresh]
        stale_sites = [s for s in all_status if not s.is_fresh]
        
        return {
            'all_status': all_status,
            'fresh_sites': fresh_sites,
            'stale_sites': stale_sites,
            'total_sites': len(all_status),
            'fresh_count': len(fresh_sites),
            'stale_count': len(stale_sites)
        }
    
    def _create_snapshot_status(
        self,
        site_name: str,
        snapshot: Dict[str, Any],
        current_time: datetime,
        priority: str,
        filename: str
    ) -> Optional[SnapshotStatus]:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆçŠ¶æ…‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ"""
        timestamp = snapshot.get('timestamp')
        if not timestamp:
            return None
        
        try:
            update_time = datetime.fromisoformat(timestamp)
            elapsed = (current_time - update_time).total_seconds()
            
            return SnapshotStatus(
                site=site_name,
                priority=priority,
                elapsed_minutes=elapsed / 60,
                is_fresh=elapsed <= self.SNAPSHOT_THRESHOLD_SECONDS,
                file=filename
            )
        except Exception as e:
            self.logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è§£æå¤±æ•— [{site_name}]: {e}")
            return None
    
    def format_report_message(self, report_data: Dict[str, Any]) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ChatWorkå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        message = "[info]"
        message += "=" * 40 + "\n"
        message += "ğŸ“Š å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ¬ãƒãƒ¼ãƒˆ\n"
        message += f"ğŸ• {timestamp}\n"
        message += "=" * 40 + "\n\n"
        
        message += "ã€çµ±è¨ˆã€‘\n"
        message += f"âœ… æ–°é®®: {report_data['fresh_count']}ä»¶ (30åˆ†ä»¥å†…)\n"
        message += f"âš ï¸ å¤ã„: {report_data['stale_count']}ä»¶ (30åˆ†è¶…é)\n"
        message += f"ğŸ“ ç·è¨ˆ: {report_data['total_sites']}ã‚µã‚¤ãƒˆ\n\n"
        
        # å¤ã„ã‚µã‚¤ãƒˆã®ã¿è©³ç´°è¡¨ç¤º
        if report_data['stale_sites']:
            message += f"ã€âš ï¸ æ›´æ–°ãŒå¤ã„ã‚µã‚¤ãƒˆ: {len(report_data['stale_sites'])}ä»¶ã€‘\n"
            sorted_stale = sorted(
                report_data['stale_sites'],
                key=lambda x: x.elapsed_minutes,
                reverse=True
            )
            for status in sorted_stale:
                elapsed_min = status.elapsed_minutes
                
                if elapsed_min < 60:
                    time_str = f"{elapsed_min:.0f}åˆ†å‰"
                else:
                    time_str = f"{elapsed_min/60:.1f}æ™‚é–“å‰"
                
                message += f"  âš ï¸ [{status.priority}] {status.site}: {time_str}\n"
            message += "\n"
        else:
            message += "âœ… å…¨ã‚µã‚¤ãƒˆæ­£å¸¸æ›´æ–°ä¸­\n\n"
        
        message += "=" * 40 + "\n"
        message += "[/info]"
        
        return message


# ==================== ChatWorké€šçŸ¥ (æ”¹ä¿®) ====================

class ChatWorkNotifier:
    """ChatWorké€šçŸ¥ã‚¯ãƒ©ã‚¹(ãƒãƒ¼ãƒ‰ã‚ªãƒ•å¯¾å¿œç‰ˆ)"""
    
    DEFAULT_ROOM_ID: Final[str] = '385402385'
    MAX_RETRIES: Final[int] = 3
    RETRY_DELAY: Final[float] = 1.0
    
    def __init__(
        self,
        token: Optional[str] = None,
        logger: logging.Logger = LOGGER
    ) -> None:
        self.token = token or os.getenv('CHATWORK_TOKEN')
        if not self.token:
            raise ValueError(
                "CHATWORK_TOKENç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
                "export CHATWORK_TOKEN='your_token_here' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )
        self.logger = logger
    
    def send_notification(
        self, message: str, room_id: str, retry: int = 0
    ) -> bool:
        if not room_id or room_id.lower() in ['nan', 'none', '']:
            return False
        
        try:
            response = requests.post(
                f"https://api.chatwork.com/v2/rooms/{room_id}/messages",
                headers={"X-ChatWorkToken": self.token},
                data={"body": message},
                timeout=HTTP_TIMEOUT
            )
            
            if response.status_code == 200:
                self.logger.info(f"ChatWorké€šçŸ¥é€ä¿¡æˆåŠŸ (ãƒ«ãƒ¼ãƒ : {room_id})")
                return True
            elif response.status_code == 429 and retry < self.MAX_RETRIES:
                time.sleep(self.RETRY_DELAY * (retry + 1))
                return self.send_notification(message, room_id, retry + 1)
            else:
                self.logger.error(
                    f"ChatWorké€šçŸ¥é€ä¿¡å¤±æ•—: {response.status_code} - {response.text}"
                )
                return False
                
        except requests.Timeout:
            if retry < self.MAX_RETRIES:
                time.sleep(self.RETRY_DELAY)
                return self.send_notification(message, room_id, retry + 1)
            ErrorHandler.handle(
                Exception("ChatWorkã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"),
                "é€šçŸ¥é€ä¿¡",
                ErrorSeverity.RECOVERABLE
            )
            return False
        except Exception as e:
            ErrorHandler.handle(e, "ChatWorké€šçŸ¥", ErrorSeverity.RECOVERABLE)
            return False
    
    def format_new_products_notification(
        self,
        display_name: str,
        category: str,
        scraping_url: str,
        products: List[Dict[str, Any]],
        url_index: int = 0  # â† è¿½åŠ 
    ) -> str:
        """æ–°å•†å“é€šçŸ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(ãƒãƒ¼ãƒ‰ã‚ªãƒ•URL indexå¯¾å¿œç‰ˆ)
        
        Args:
            display_name: ã‚µã‚¤ãƒˆè¡¨ç¤ºå
            category: ã‚«ãƒ†ã‚´ãƒªå
            scraping_url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
            products: å•†å“ãƒªã‚¹ãƒˆ
            url_index: URL index (ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã®åˆ¤å®šã«ä½¿ç”¨)
        """
        message = "[info]"
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        message += f"ğŸ”” {display_name} + {category}\n"
        message += "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«\n"
        message += f"ğŸ”— {scraping_url}\n"
        message += "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«\n\n"
        
        # ãƒãƒ¼ãƒ‰ã‚ªãƒ•åˆ¤å®šï¼ˆURL indexè€ƒæ…®ï¼‰
        use_hardoff_format = HardOffFormatter.should_use_new_format(
            display_name, scraping_url, url_index
        )
        
        for product in products[:20]:
            if use_hardoff_format:
                # ãƒãƒ¼ãƒ‰ã‚ªãƒ•ã‚«ãƒ¡ãƒ©ç³»: æ–°å½¢å¼
                message += HardOffFormatter.format_product_line(product)
            else:
                # å¾“æ¥å½¢å¼ï¼ˆãƒãƒ¼ãƒ‰ã‚ªãƒ•æ™‚è¨ˆ or ãã®ä»–ã‚µã‚¤ãƒˆï¼‰
                price_text = (
                    f"{product['price']}å††"
                    if product.get('price', '0') != '0'
                    else "ãŠå•ã„åˆã‚ã›"
                )
                message += f"â–ª {product['name']}ãƒ»{price_text}\n\n"
        
        if len(products) > 20:
            message += f"...ä»–{len(products) - 20}ä»¶\n"
        
        message += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/info]"
        return message


# ==================== Playwrightä¸¦åˆ—åˆ¶å¾¡ ====================

class PlaywrightSemaphore:
    """Playwrightä¸¦åˆ—å®Ÿè¡Œåˆ¶å¾¡"""
    
    def __init__(self, max_concurrent: int = 2, logger: logging.Logger = LOGGER) -> None:
        self.semaphore = threading.Semaphore(max_concurrent)
        self.logger = logger
    
    def acquire(self, script_name: str) -> bool:
        try:
            acquired = self.semaphore.acquire(timeout=5)
            if acquired:
                self.logger.info(f"Playwrightä¸¦åˆ—åˆ¶å¾¡: {script_name} é–‹å§‹")
            return acquired
        except Exception as e:
            ErrorHandler.handle(e, "Playwrightã‚»ãƒãƒ•ã‚©å–å¾—", ErrorSeverity.EXPECTED)
            return False
    
    def release(self, script_name: str) -> None:
        try:
            self.semaphore.release()
            self.logger.info(f"Playwrightä¸¦åˆ—åˆ¶å¾¡: {script_name} å®Œäº†")
        except Exception as e:
            ErrorHandler.handle(e, "Playwrightã‚»ãƒãƒ•ã‚©è§£æ”¾", ErrorSeverity.EXPECTED)


# ==================== è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† ====================

class SafeCSVManager:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_file: str = "shop_config.json", logger: logging.Logger = LOGGER) -> None:
        self.config_file = config_file
        self.priority1_scripts: List[str] = []
        self.priority2_scripts: List[str] = []
        self.notification_config: Dict[str, Dict[str, Any]] = {}
        self.url_config_mapping: Dict[str, List[Dict[str, Any]]] = {}
        self.script_priority_mapping: Dict[str, int] = {}  # v26è¿½åŠ 
        self.logger = logger
        self.load_config()
    
    def load_config(self) -> None:
        if not Path(self.config_file).exists():
            self.logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_file}")
            return
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            if df.empty:
                return
            
            df['priority'] = pd.to_numeric(df['priority'], errors='coerce').fillna(2).astype(int)
            df['is_active'] = df['is_active'].astype(str).str.lower().map(
                {'true': True, 'false': False}
            ).fillna(False)
            df['url_index'] = pd.to_numeric(
                df.get('url_index', 0), errors='coerce'
            ).fillna(0).astype(int)
            
            active_scripts = df[df['is_active'] == True]
            
            self.priority1_scripts = active_scripts[
                active_scripts['priority'] == 1
            ]['py_file'].unique().tolist()
            
            self.priority2_scripts = active_scripts[
                active_scripts['priority'] > 1
            ]['py_file'].unique().tolist()
            
            # v26è¿½åŠ : ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°æ§‹ç¯‰
            for _, row in active_scripts.iterrows():
                py_file = row['py_file']
                priority = int(row['priority'])
                self.script_priority_mapping[py_file] = priority
                self._process_config_row(row)
            
            self.logger.info(
                f"è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: "
                f"P1={len(self.priority1_scripts)}ä»¶, "
                f"P2={len(self.priority2_scripts)}ä»¶"
            )
            
        except Exception as e:
            ErrorHandler.handle(e, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿", ErrorSeverity.FATAL)
    
    def _process_config_row(self, row: pd.Series) -> None:
        py_file = row['py_file']
        
        notification_ids = str(row['notification_enabled']).strip() if (
            'notification_enabled' in row and pd.notna(row['notification_enabled'])
        ) else ''
        
        if notification_ids.lower() == 'true':
            notification_ids = ChatWorkNotifier.DEFAULT_ROOM_ID
        
        config_entry = {
            'notification_room_ids': notification_ids if notification_ids.lower() not in [
                'nan', 'non', 'none', 'false', ''
            ] else None,
            'display_name': str(row['display_name']),
            'category': str(row.get('category', 'æ–°ç€')),
            'scraping_url': str(row.get('scraping_url', '')),
            'url_index': int(row['url_index']),
            'priority': int(row['priority'])  # v26è¿½åŠ 
        }
        
        if py_file not in self.url_config_mapping:
            self.url_config_mapping[py_file] = []
        
        self.url_config_mapping[py_file].append(config_entry)
        
        if config_entry['url_index'] == 0:
            self.notification_config[py_file] = config_entry
    
    def get_priority1_scripts(self) -> List[str]:
        return self.priority1_scripts
    
    def get_priority2_scripts(self) -> List[str]:
        return self.priority2_scripts
    
    def get_all_url_configs(self, py_file: str) -> List[Dict[str, Any]]:
        return self.url_config_mapping.get(py_file, [])
    
    def get_script_priority_mapping(self) -> Dict[str, int]:
        """v26è¿½åŠ : ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—"""
        return self.script_priority_mapping


# ==================== éåŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå™¨ ====================

class AsyncStableExecutor:
        
    def __init__(
        self,
        blocked_scripts: Set[str],
        memory_system: SimpleMemoryDiffSystem,
        playwright_semaphore: PlaywrightSemaphore,
        csv_manager: SafeCSVManager,
        chatwork_notifier: ChatWorkNotifier,
        logger: logging.Logger = LOGGER
    ) -> None:
        self.logger = logger
        self.extractor = StableDataExtractor(logger)
        self.blocked_scripts = blocked_scripts
        self.memory_system = memory_system
        self.playwright_semaphore = playwright_semaphore
        self.csv_manager = csv_manager
        self.chatwork_notifier = chatwork_notifier
        self.running = False
        self.stats: Dict[str, int] = {}
        self.error_log: List[Dict[str, str]] = []
    
    async def execute_async(self, script: str) -> ExecutionResult:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.execute_stable, script)
    
    def is_playwright_script(self, script_path: str) -> bool:
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            playwright_patterns = ['playwright', 'async_playwright', 'browser.new_page']
            return any(pattern in content for pattern in playwright_patterns)
        except Exception:
            return False
    
    def execute_stable(self, script: str) -> ExecutionResult:
        script_path = Path(script)
        
        if not script_path.exists():
            return ExecutionResult(
                success=False,
                error='ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨',
                data={'count': 0}
            )
        
        script_name = script_path.name
        
        if script_name in self.blocked_scripts:
            self.logger.warning(f"[{script_name}] ãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return ExecutionResult(
                success=False,
                error='ãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡',
                data={'count': 0}
            )
        
        is_playwright = self.is_playwright_script(str(script_path))
        acquired_semaphore = False
        
        if is_playwright:
            acquired_semaphore = self.playwright_semaphore.acquire(script_name)
            if not acquired_semaphore:
                self.logger.warning(f"[{script_name}] Playwrightä¸¦åˆ—åˆ¶é™ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—")
                return ExecutionResult(
                    success=True,
                    data={'count': 0}
                )
        
        self.logger.info(f"[{script_name}] å®Ÿè¡Œé–‹å§‹")
        start_time = time.time()
        
        try:
            result = self._run_subprocess(script_path)
            duration = time.time() - start_time
            
            data_result = self.extractor.extract_stable(result.stdout, script_name)
            success = result.returncode == 0 and data_result['success']
            
            if success and data_result.get('products'):
                self.logger.info(
                    f"[{script_name}] æˆåŠŸ ({duration:.1f}s) - {data_result['count']}ä»¶å–å¾—"
                )
                self.process_products_by_url_index(script_name, data_result['products'])
            elif not success:
                self.logger.error(f"[{script_name}] å¤±æ•— ({duration:.1f}s)")
            else:
                self.logger.info(f"[{script_name}] æˆåŠŸ ({duration:.1f}s) - 0ä»¶")
            
            return ExecutionResult(
                success=success,
                duration=duration,
                data=data_result
            )
            
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            self.logger.warning(f"[{script_name}] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({SUBPROCESS_TIMEOUT}s)")
            self.error_log.append({'script': script_name, 'error': f'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ({SUBPROCESS_TIMEOUT}s)'})
            return ExecutionResult(
                success=False,
                duration=duration,
                error=f'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ({SUBPROCESS_TIMEOUT}s)',
                data={'count': 0}
            )
        except Exception as e:
            duration = time.time() - start_time
            ErrorHandler.handle(e, f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ[{script_name}]", ErrorSeverity.RECOVERABLE)
            self.error_log.append({'script': script_name, 'error': str(e)})
            return ExecutionResult(
                success=False,
                duration=duration,
                error=str(e),
                data={'count': 0}
            )
        finally:
            if acquired_semaphore:
                self.playwright_semaphore.release(script_name)
    
    def _run_subprocess(self, script_path: Path) -> subprocess.CompletedProcess:
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['PYTHONUNBUFFERED'] = '1'
        
        return subprocess.run(
            [sys.executable, '-u', str(script_path)],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            timeout=SUBPROCESS_TIMEOUT,
            cwd=str(script_path.parent),
            env=env
        )
    
    def process_products_by_url_index(
        self,
        script_name: str,
        all_products: List[Dict[str, Any]]
    ) -> None:
        all_url_configs = self.csv_manager.get_all_url_configs(script_name)
        
        if not all_url_configs:
            self.logger.warning(f"[{script_name}] URLè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        products_by_index: Dict[int, List[Dict[str, Any]]] = {}
        for product in all_products:
            url_idx = product.get('url_index', 0)
            if url_idx not in products_by_index:
                products_by_index[url_idx] = []
            products_by_index[url_idx].append(product)
        
        self.logger.info(
            f"[{script_name}] URLåˆ¥å•†å“æ•°: "
            f"{[(idx, len(prods)) for idx, prods in products_by_index.items()]}"
        )
        
        for url_index, url_products in products_by_index.items():
            matching_configs = [c for c in all_url_configs if c['url_index'] == url_index]
            if not matching_configs:
                continue
            
            url_config = matching_configs[0]
            display_name = url_config['display_name']
            category = url_config['category']
            unique_display_name = f"{display_name}_{category}"
            
            self.logger.info(
                f"[{display_name}] URL index {url_index} ({category}): "
                f"{len(url_products)}ä»¶"
            )
            
            scraping_url = url_config.get('scraping_url', '')
            
            # v26: script_nameã¨url_indexã‚’æ¸¡ã™
            new_products = self.memory_system.detect_new_products(
                unique_display_name,
                url_products,
                scraping_url=scraping_url,
                script_name=script_name,
                url_index=url_index
            )
            
            if new_products:
                self.send_notification_for_url(script_name, url_config, new_products, display_name)
    
    def send_notification_for_url(
        self,
        script_name: str,
        url_config: Dict[str, Any],
        new_products: List[Dict[str, Any]],
        display_name: Optional[str] = None
    ) -> None:
        if display_name is None:
            display_name = url_config['display_name']
        
        category = url_config['category']
        scraping_url = url_config['scraping_url']
        url_index = url_config.get('url_index', 0)  # â† URL index ã‚’å–å¾—
        
        # URL index ã‚’æ¸¡ã™
        message = self.chatwork_notifier.format_new_products_notification(
            display_name, category, scraping_url, new_products, url_index
        )
        
        notification_ids = url_config.get('notification_room_ids')
        if notification_ids:
            room_ids = [r.strip() for r in str(notification_ids).split(',') if r.strip()]
            for room_id in room_ids:
                self.logger.info(f"[{display_name}] ChatWorké€šçŸ¥é€ä¿¡: ãƒ«ãƒ¼ãƒ  {room_id}")
                self.chatwork_notifier.send_notification(message, room_id)
        
        if hasattr(self, 'master_controller'):
            self.master_controller.last_new_product_time[script_name] = datetime.now()


# ==================== Priority2ã‚µã‚¤ã‚¯ãƒ«åˆ¶å¾¡ (ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ) ====================

class Tier2CycleController:
    """Priority2ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œåˆ¶å¾¡(ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ)"""
    
    REPORT_INTERVAL_SECONDS: Final[int] = 1800  # 30åˆ†
    SNAPSHOT_THRESHOLD_SECONDS: Final[int] = 1800
    ADMIN_ROOM_ID: Final[str] = "413142921"
    
    def __init__(
        self,
        scripts: List[str],
        executor: AsyncStableExecutor,
        memory_system: SimpleMemoryDiffSystem,
        chatwork_notifier: ChatWorkNotifier,
        logger: logging.Logger = LOGGER
    ) -> None:
        self.scripts = scripts
        self.executor = executor
        self.memory_system = memory_system
        self.chatwork_notifier = chatwork_notifier
        self.logger = logger
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–
        self.report_generator = SnapshotReportGenerator(memory_system, logger)
        
        self.script_queue: List[str] = []
        self.queue_lock = threading.Lock()
        self.cycle_start_time = datetime.now()
        self.executed_scripts: Set[str] = set()
        self.cycle_count = 0
        self.last_report_time = datetime.now()
        
        self.script_to_snapshot_keys: Dict[str, List[str]] = {}
        self._build_snapshot_key_mapping()
        
        with self.queue_lock:
            self.script_queue = list(scripts)
        
        self.logger.info(
            f"å®Œå…¨å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–: {len(scripts)}ä»¶ - å…¨ä»¶å®Ÿè¡Œå®Œäº†ã¾ã§æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¾…æ©Ÿ"
        )
    
    def _build_snapshot_key_mapping(self) -> None:
        for script in self.scripts:
            all_configs = self.executor.csv_manager.get_all_url_configs(script)
            
            snapshot_keys = []
            for config in all_configs:
                display_name = config['display_name']
                category = config['category']
                key = f"{display_name}_{category}"
                snapshot_keys.append(key)
            
            self.script_to_snapshot_keys[script] = snapshot_keys
            
            if snapshot_keys:
                self.logger.debug(f"[{script}] ãƒãƒƒãƒ”ãƒ³ã‚°: {snapshot_keys}")
    
    def should_send_report(self) -> bool:
        """30åˆ†ã”ã¨ã«ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡"""
        elapsed = (datetime.now() - self.last_report_time).total_seconds()
        return elapsed >= self.REPORT_INTERVAL_SECONDS
    
    def send_comprehensive_snapshot_report(self) -> None:
        """å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡"""
        try:
            report_data = self.report_generator.generate_comprehensive_report()
            message = self.report_generator.format_report_message(report_data)
            
            self.chatwork_notifier.send_notification(message, self.ADMIN_ROOM_ID)
            self.last_report_time = datetime.now()
            
            self.logger.info(
                f"ğŸ“Š å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡å®Œäº†: "
                f"æ–°é®®{report_data['fresh_count']}/å¤ã„{report_data['stale_count']}"
            )
            
        except Exception as e:
            ErrorHandler.handle(e, "å…¨URLãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡", ErrorSeverity.RECOVERABLE)
    
    async def run_cycle_async(self) -> None:
        """ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ(30åˆ†ã”ã¨ã®ãƒ¬ãƒãƒ¼ãƒˆè¿½åŠ )"""
        running_tasks: List[asyncio.Task] = []
        MAX_CONCURRENT_P2 = 1
        
        while self.executor.running:
            now = datetime.now()
            is_night = TimeManager.is_nighttime()
            
            # 30åˆ†ã”ã¨ã®ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡ãƒã‚§ãƒƒã‚¯
            if self.should_send_report():
                self.send_comprehensive_snapshot_report()
            
            running_tasks = [t for t in running_tasks if not t.done()]
            
            if not self.script_queue and not running_tasks:
                self.cycle_count += 1
                cycle_duration = (now - self.cycle_start_time).total_seconds()
                
                not_executed = set(self.scripts) - self.executed_scripts
                
                if not_executed:
                    self.logger.warning(f"âš ï¸ æœªå®Ÿè¡Œ: {len(not_executed)}ä»¶")
                
                self.logger.info(
                    f"âœ… ã‚µã‚¤ã‚¯ãƒ«{self.cycle_count}å®Œäº†: "
                    f"{cycle_duration:.0f}ç§’ ({len(self.executed_scripts)}ä»¶å®Ÿè¡Œ)"
                )
                
                # æ·±å¤œã¯å…¨ä½“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¾Œã«ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
                if is_night:
                    self.logger.info("ğŸŒ™ æ·±å¤œã‚µã‚¤ã‚¯ãƒ«å®Œäº† - å…¨URLãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡")
                    self.send_comprehensive_snapshot_report()
                
                with self.queue_lock:
                    retry_scripts = list(not_executed)
                    normal_scripts = [s for s in self.scripts if s not in retry_scripts]
                    
                    self.script_queue = retry_scripts + normal_scripts
                    
                    if retry_scripts:
                        self.logger.info(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤å„ªå…ˆ: {len(retry_scripts)}ä»¶")
                        for script in retry_scripts:
                            self.logger.info(f"   ğŸ“Œ {script}")
                    
                    self.executed_scripts = set()
                    
                    target_interval = TimeManager.get_interval_for_priority2(is_night)
                    wait_time = max(5, target_interval - cycle_duration)
                    
                    time_label = "æ·±å¤œ" if is_night else "é€šå¸¸"
                    self.logger.info(
                        f"ğŸ”„ ã‚µã‚¤ã‚¯ãƒ«{self.cycle_count + 1}é–‹å§‹äºˆå®š: "
                        f"{wait_time:.0f}ç§’å¾Œ ({time_label}æ™‚é–“å¸¯: {target_interval}ç§’é–“éš”)"
                    )
                    self.cycle_start_time = now
                
                self.logger.info(
                    f"â° æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã¾ã§ {wait_time:.0f}ç§’å¾…æ©Ÿ... ({wait_time/60:.1f}åˆ†)"
                )
                await asyncio.sleep(wait_time)
                continue
            
            while len(running_tasks) < MAX_CONCURRENT_P2:
                with self.queue_lock:
                    if not self.script_queue:
                        break
                    
                    script_to_execute = self.script_queue.pop(0)
                    remaining = len(self.script_queue)
                    elapsed = (now - self.cycle_start_time).total_seconds()
                    
                    self.logger.info(
                        f"Tier2å®Ÿè¡Œ: {script_to_execute} "
                        f"(æ®‹ã‚Š={remaining}ä»¶, çµŒé={elapsed:.0f}ç§’)"
                    )
                    
                    task = asyncio.create_task(self._execute_and_record(script_to_execute))
                    running_tasks.append(task)
            
            await asyncio.sleep(1)
    
    async def _execute_and_record(self, script: str) -> ExecutionResult:
        result = await self.executor.execute_async(script)
        
        self.executed_scripts.add(script)
        
        if result.success:
            self.executor.stats['total_executions'] += 1
            self.executor.stats['successful_executions'] += 1
            self.executor.stats['total_products'] += result.data['count']
        elif not isinstance(result, Exception):
            self.executor.stats['total_executions'] += 1
        
        return result


# ==================== ãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ ====================

class FinalStableMasterController:
    """æœ€çµ‚å®‰å®šç‰ˆãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ v27(P1å€‹åˆ¥ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¯¾å¿œ)"""
    
    VERSION: Final[str] = "27"
    
    def __init__(self) -> None:
        self.running = False
        self.start_time: Optional[datetime] = None
        self.blocked_scripts: Set[str] = set()
        self.logger = LOGGER
        
        self.script_intervals: Dict[str, int] = {}
        self.last_new_product_time: Dict[str, datetime] = {}
        
        self.csv_manager = SafeCSVManager("shop_config.json", self.logger)
        
        # v26: SimpleMemoryDiffSystemã«ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¨­å®š
        self.memory_system = SimpleMemoryDiffSystem(logger=self.logger)
        self.memory_system.set_priority_mapping(self.csv_manager.get_script_priority_mapping())
        
        self.chatwork_notifier = ChatWorkNotifier(logger=self.logger)
        self.playwright_semaphore = PlaywrightSemaphore(max_concurrent=3, logger=self.logger)
        
        self.executor = AsyncStableExecutor(
            self.blocked_scripts,
            self.memory_system,
            self.playwright_semaphore,
            self.csv_manager,
            self.chatwork_notifier,
            self.logger
        )
        
        self.executor.master_controller = self
        
        priority2_scripts = self.csv_manager.get_priority2_scripts()
        self.tier2_controller = Tier2CycleController(
            priority2_scripts,
            self.executor,
            self.memory_system,
            self.chatwork_notifier,
            self.logger
        )
        
        self.stats = {
            'cycles': 0,
            'total_executions': 0,
            'successful_executions': 0,
            'total_products': 0,
            'blocked_scripts': len(self.blocked_scripts)
        }
        self.executor.stats = self.stats
        
        signal.signal(signal.SIGINT, self.signal_handler)
    
    def signal_handler(self, signum: int, frame: Any) -> None:
        self.logger.info(f"çµ‚äº†ã‚·ã‚°ãƒŠãƒ«å—ä¿¡: {signum}")
        self.stop()
    
    def stop(self) -> None:
        self.running = False
        self.executor.running = False
        
        if isinstance(self.memory_system.notification_manager, SQLiteNotificationHistory):
            self.logger.info("SQLiteé€šçŸ¥å±¥æ­´ã¯è‡ªå‹•ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
        
        self.logger.info("ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")
    
    async def execute_priority1_async(self) -> None:
        priority1_scripts = self.csv_manager.get_priority1_scripts()
        
        if not priority1_scripts:
            return
        
        last_run_times = {script: datetime.min for script in priority1_scripts}
        
        is_night_initial = TimeManager.is_nighttime()
        for script in priority1_scripts:
            if is_night_initial:
                self.script_intervals[script] = TimeManager.NIGHT_INTERVAL_SECONDS
                self.logger.info(
                    f"[{script}] åˆæœŸé–“éš”: {TimeManager.NIGHT_INTERVAL_SECONDS}ç§’(æ·±å¤œèµ·å‹•)"
                )
            else:
                self.script_intervals[script] = 60
            self.last_new_product_time[script] = datetime.now() - timedelta(hours=2)
        
        while self.running:
            now = datetime.now()
            is_night = TimeManager.is_nighttime()
            
            tasks = []
            
            for script in priority1_scripts:
                if not self.running:
                    break
                
                last_new_product = self.last_new_product_time.get(script, now)
                idle_time = (now - last_new_product).total_seconds()
                
                interval = TimeManager.get_interval_for_priority1(idle_time, is_night)
                
                if self.script_intervals.get(script) != interval:
                    reason = "æ·±å¤œå›ºå®š" if is_night else f"ç„¡æ›´æ–°: {idle_time/60:.1f}åˆ†"
                    self.logger.info(
                        f"[{script}] é–“éš”å¤‰æ›´: "
                        f"{self.script_intervals.get(script, 60)}ç§’ â†’ {interval}ç§’ "
                        f"(ç†ç”±: {reason})"
                    )
                    self.script_intervals[script] = interval
                
                elapsed = (now - last_run_times[script]).total_seconds()
                if elapsed >= interval:
                    last_run_times[script] = now
                    tasks.append(self._execute_with_interval_tracking(script))
            
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if not isinstance(result, Exception) and result.success:
                        self.stats['total_executions'] += 1
                        self.stats['successful_executions'] += 1
                        self.stats['total_products'] += result.data['count']
                    elif not isinstance(result, Exception):
                        self.stats['total_executions'] += 1
            
            await asyncio.sleep(5)
    
    async def _execute_with_interval_tracking(self, script: str) -> ExecutionResult:
        result = await self.executor.execute_async(script)
        
        if result.success and result.data.get('count', 0) > 0:
            self.last_new_product_time[script] = datetime.now()
        
        return result
    
    async def execute_cycle_async(self) -> None:
        while self.running:
            self.stats['cycles'] += 1
            cycle_start = time.time()
            
            self.logger.info(f"=== ã‚µã‚¤ã‚¯ãƒ« {self.stats['cycles']} é–‹å§‹ ===")
            
            mem_stats = self.memory_system.get_snapshot_stats()
            cycle_duration = time.time() - cycle_start
            
            self.logger.info(
                f"=== ã‚µã‚¤ã‚¯ãƒ« {self.stats['cycles']} å®Œäº† ({cycle_duration:.1f}s) ==="
            )
            
            if self.stats['cycles'] % 3 == 0:
                executed = self.stats['total_executions'] - self.stats['blocked_scripts']
                success_rate = (
                    (self.stats['successful_executions'] / max(1, executed)) * 100
                )
                self.logger.info("--- çµ±è¨ˆ ---")
                self.logger.info(f"ç·å®Ÿè¡Œ: {executed}, æˆåŠŸç‡: {success_rate:.1f}%")
                self.logger.info(f"ç´¯è¨ˆå•†å“: {self.stats['total_products']}ä»¶")
                self.logger.info(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: P1={mem_stats['p1_files']}ãƒ•ã‚¡ã‚¤ãƒ«, P2å…±æœ‰={mem_stats['p2_shared']}")
            
            for i in range(300):
                if not self.running:
                    break
                await asyncio.sleep(1)
    
    async def start_async(self) -> None:
        self.running = True
        self.executor.running = True
        self.start_time = datetime.now()
        
        print("=" * 60)
        print(f"Master Controller v{self.VERSION} - P1å€‹åˆ¥ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¯¾å¿œç‰ˆ")
        print("=" * 60)
        print("ğŸ”§ v27æ–°æ©Ÿèƒ½: å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç›£è¦–(30åˆ†ã”ã¨)")
        print("ğŸ”§ v27æ–°æ©Ÿèƒ½: ãƒãƒ¼ãƒ‰ã‚ªãƒ•å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ")
        print("ğŸ“ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: snapshots/")
        print("   P1: snapshots/p1_{script}_{url_index}.json(å€‹åˆ¥)")
        print("   P2: snapshots/p2_shared.json(å…±æœ‰)")
        print("")
        print(f"æ·±å¤œé–“éš”: {TimeManager.NIGHT_INTERVAL_SECONDS}ç§’ = {TimeManager.NIGHT_INTERVAL_SECONDS/60:.0f}åˆ†")
        print("ğŸ“Š é€šçŸ¥å±¥æ­´: SQLiteå½¢å¼(æ¨å¥¨ç‰ˆãƒ»ACIDä¿è¨¼ãƒ»WALãƒ¢ãƒ¼ãƒ‰)")
        print("Ctrl+C ã§åœæ­¢")
        print("=" * 60)
        
        priority1_scripts = self.csv_manager.get_priority1_scripts()
        priority2_scripts = self.csv_manager.get_priority2_scripts()
        
        self.logger.info("å®Ÿè¡Œå¯¾è±¡:")
        self.logger.info(
            f"  å„ªå…ˆåº¦1: {len(priority1_scripts)}ä»¶"
            f"(å‹•çš„é–“éš”: 60ç§’ã€œ1æ™‚é–“ã€æ·±å¤œ: 30åˆ†å›ºå®š) â†’ å€‹åˆ¥JSONãƒ•ã‚¡ã‚¤ãƒ«"
        )
        self.logger.info(
            f"  å„ªå…ˆåº¦2: {len(priority2_scripts)}ä»¶"
            f"(5åˆ†å›ºå®šã€æ·±å¤œ: 30åˆ†å›ºå®š) â†’ å…±æœ‰JSONãƒ•ã‚¡ã‚¤ãƒ«"
        )
        
        if self.blocked_scripts:
            self.logger.info(f"ãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡: {', '.join(self.blocked_scripts)}")
        
        try:
            tasks = []
            
            if priority1_scripts:
                tasks.append(asyncio.create_task(self.execute_priority1_async()))
            
            if priority2_scripts:
                tasks.append(asyncio.create_task(self.tier2_controller.run_cycle_async()))
            
            tasks.append(asyncio.create_task(self.execute_cycle_async()))
            
            await asyncio.gather(*tasks)
        
        except KeyboardInterrupt:
            self.logger.info("ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿")
        except Exception as e:
            ErrorHandler.handle(e, "ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ", ErrorSeverity.FATAL)
        
        finally:
            self.stop()
            executed = self.stats['total_executions'] - self.stats['blocked_scripts']
            if executed > 0:
                success_rate = (
                    (self.stats['successful_executions'] / executed) * 100
                )
                self.logger.info(
                    f"æœ€çµ‚çµ±è¨ˆ: æˆåŠŸç‡{success_rate:.1f}%, "
                    f"ç´¯è¨ˆå•†å“{self.stats['total_products']}ä»¶"
                )
            
            mem_stats = self.memory_system.get_snapshot_stats()
            self.logger.info(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: {mem_stats['total_sites']}ã‚µã‚¤ãƒˆ")
            self.logger.info("ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†")
    
    def start(self) -> None:
        asyncio.run(self.start_async())


# ==================== ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ====================

def main() -> None:
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    print(f"Master Controller v{FinalStableMasterController.VERSION}")
    print("=" * 40)
    print("ğŸ”§ v27 - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Œäº†ç‰ˆ")
    print("=" * 40)
    print("")
    print("ã€ä¸»ãªæ”¹å–„ç‚¹ã€‘")
    print("âœ… ã‚¯ãƒ©ã‚¹ã®å˜ä¸€è²¬ä»»åŸå‰‡å¾¹åº•")
    print("âœ… ãƒãƒ¼ãƒ‰ã‚ªãƒ•ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼åˆ†é›¢")
    print("âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨å°‚ç”¨ã‚¯ãƒ©ã‚¹åŒ–")
    print("âœ… ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹æ´»ç”¨ã§å‹å®‰å…¨æ€§å‘ä¸Š")
    print("âœ… 30åˆ†ã”ã¨ã®å…¨URLã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç›£è¦–")
    print("")
    
    controller = FinalStableMasterController()
    controller.start()


if __name__ == "__main__":
    main()